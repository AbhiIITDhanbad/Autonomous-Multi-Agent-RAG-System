AI Infrastructure, Accelerators, and Cloud Compute Economics for Large Language Models (2022–Q1 2024)

Large language model (LLM) training and deployment are fundamentally constrained by computational infrastructure. As model sizes scale into tens and hundreds of billions of parameters, the limiting factors are no longer algorithmic ideas alone but the availability, performance, memory capacity, interconnect bandwidth, and cost structure of AI accelerators. Understanding modern AI infrastructure therefore requires a detailed examination of accelerator architectures, performance metrics, cloud pricing models, and benchmarking practices.

This document focuses on high-end AI accelerators used for LLM training, with particular emphasis on GPUs and tensor processors offered by NVIDIA, AMD, and Google Cloud. It also examines how these accelerators are priced and deployed on major cloud platforms, especially AWS, as of Q1 2024.

1. Core Performance Metrics for AI Accelerators

AI accelerators are evaluated using a combination of compute throughput, memory performance, and system-level efficiency.

1.1 Compute Throughput (TFLOPS)

Compute performance is commonly expressed in TFLOPS (tera–floating-point operations per second). For modern LLM workloads, reduced-precision formats such as FP16, BF16, and FP8 are more relevant than FP32, because transformer models are designed to tolerate lower precision.

FP8 TFLOPS is particularly important for next-generation training, as it enables higher throughput without significant loss of accuracy.

BF16 TFLOPS is widely used in Google’s TPU ecosystem and in mixed-precision training workflows.

Peak TFLOPS numbers represent theoretical maximums under ideal conditions and do not necessarily reflect real-world performance.

1.2 Memory Bandwidth

As model sizes increase, memory bandwidth becomes as important as raw compute. Transformers require continuous movement of large tensors between memory and compute units.

Memory bandwidth is measured in terabytes per second (TB/s).

Insufficient bandwidth leads to compute starvation, where execution units sit idle waiting for data.

High-bandwidth memory (HBM) technologies such as HBM3 and HBM3e are therefore critical for large-scale training.

1.3 Cost per Hour (Cloud Economics)

From a practical perspective, cost per training hour is often more important than peak performance. Cloud providers bundle accelerators into instance types that include networking, storage, and system software.

Key considerations include:

On-demand hourly pricing

Availability on specific cloud platforms

Scalability across multi-node clusters

Long-running job stability (non-preemptible instances)

2. NVIDIA H200 Tensor Core GPU

The NVIDIA H200 is part of NVIDIA’s Hopper architecture and represents the company’s flagship accelerator for large-scale AI training as of late 2023 and early 2024.

2.1 Architecture and Performance

The H200 supports FP8 precision and delivers approximately:

~1,979 TFLOPS (FP8) theoretical peak performance

This makes it one of the highest single-chip AI accelerators available for LLM training.

2.2 Memory Subsystem

The H200 integrates HBM3e memory, providing approximately:

~4.8 TB/s memory bandwidth

This bandwidth is designed to keep the tensor cores saturated during large transformer workloads.

2.3 Cloud Availability and Pricing

As of Q1 2024, the NVIDIA H200 is available on Amazon Web Services (AWS) through the p5 instance family.

Instance type: p5.48xlarge

Hourly price: approximately $98.32 per hour

This price reflects on-demand usage and does not include long-term reserved instance discounts.

2.4 Practical Considerations

NVIDIA benefits from a mature software ecosystem (CUDA, cuDNN, NCCL).

H200 systems are commonly used for multi-node distributed training.

AWS availability makes H200 attractive for organizations already invested in AWS infrastructure.

3. AMD Instinct MI300X

The AMD Instinct MI300X represents AMD’s entry into the highest tier of AI accelerators, designed to compete directly with NVIDIA’s Hopper-based GPUs.

3.1 Architecture and Performance

The MI300X supports FP8 and FP16 operations and delivers approximately:

~1,835 TFLOPS (FP8) theoretical peak performance

While slightly lower than the H200 in raw FP8 TFLOPS, the MI300X emphasizes memory capacity and bandwidth.

3.2 Memory Subsystem

One of the MI300X’s defining features is its memory performance:

~5.2 TB/s memory bandwidth

This is higher than the H200, making the MI300X particularly attractive for memory-intensive workloads such as large transformer models with long context windows.

3.3 Cloud Availability and Pricing

As of Q1 2024:

The MI300X is not available on AWS

Comparable deployments exist on Microsoft Azure

On Azure, similar high-end AI instances using MI300X-class hardware are priced at approximately:

~$89–90 per hour (depending on configuration and region)

Because AWS does not offer MI300X instances, direct cost-per-hour comparisons on AWS are not possible.

3.4 Practical Considerations

AMD’s ROCm software stack has improved significantly but remains less mature than CUDA.

MI300X is attractive for organizations prioritizing memory bandwidth and capacity.

Cloud availability is currently more limited compared to NVIDIA GPUs.

4. Google Cloud TPU v5e

Google’s Tensor Processing Units (TPUs) represent a different design philosophy compared to GPUs. Rather than maximizing single-chip performance, TPUs are optimized for scalable, cost-efficient training across large clusters.

4.1 TPU v5e Performance Characteristics

The TPU v5e is positioned as a cost-optimized TPU for large-scale training and inference.

Each TPU v5e chip delivers approximately:

~393 TFLOPS (bfloat16)

This number is significantly lower than H200 or MI300X on a per-chip basis, but TPU performance is designed to scale linearly when deployed in large pods.

4.2 Memory and Architecture

TPUs use a tightly integrated memory and interconnect design optimized for:

Systolic array operations

Large-scale data parallelism

Deterministic performance at scale

Memory bandwidth figures are not directly comparable to GPU HBM bandwidth due to architectural differences.

4.3 Pricing Model

As of Q1 2024, the hourly cost per TPU v5e chip on Google Cloud is approximately:

$1.20 per hour per chip

However, TPU v5e chips are typically used in pods consisting of hundreds of chips, meaning total job cost depends on pod size and runtime.

4.4 Practical Considerations

TPUs are not available on AWS.

TPU performance is best realized in workloads specifically optimized for Google’s XLA and TPU software stack.

TPUs offer excellent cost efficiency at scale, but less flexibility for heterogeneous workloads.

5. Comparative Analysis: H200 vs MI300X vs TPU v5e
5.1 Compute Performance

Highest single-chip TFLOPS: NVIDIA H200 (~1,979 FP8)

Close competitor: AMD MI300X (~1,835 FP8)

Lower per-chip performance: TPU v5e (~393 BF16)

5.2 Memory Bandwidth

Highest memory bandwidth: AMD MI300X (~5.2 TB/s)

Second highest: NVIDIA H200 (~4.8 TB/s)

TPU v5e: architecture-specific bandwidth, not directly comparable

5.3 Cost and Availability

AWS availability: NVIDIA H200 only

AWS hourly cost: H200 at ~$98.32/hour

Lowest cost per chip: TPU v5e at ~$1.20/hour (Google Cloud)

MI300X: not available on AWS; Azure pricing ~$89–90/hour

6. Benchmarking and Verification (MLPerf Context)

Independent benchmarking is essential for validating performance claims. MLPerf is the industry-standard benchmark suite for AI training and inference.

NVIDIA GPUs regularly appear in MLPerf Training and Inference submissions.

Google TPUs also participate in MLPerf benchmarks.

Some proprietary or emerging accelerators do not always submit results, making independent verification difficult.

Peak TFLOPS numbers should therefore be interpreted as theoretical maxima, not guaranteed real-world throughput.

7. Cloud Economics and Strategic Implications

The rapid growth of AI workloads has changed cloud economics:

AI workloads are growing faster than general cloud workloads.

Accelerator scarcity influences pricing and availability.

Enterprises increasingly select cloud providers based on AI hardware access, not just storage or compute.

For organizations training LLMs:

NVIDIA H200 is the most powerful and AWS-accessible option.

AMD MI300X offers superior memory bandwidth but limited cloud availability.

TPU v5e provides unmatched cost efficiency at scale on Google Cloud.