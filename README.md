# ğŸš€ Autonomous Multi-Agent RAG System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![LangGraph](https://img.shields.io/badge/LangGraph-Orchestration-green.svg)](https://github.com/langchain-ai/langgraph)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **A production-grade, intelligent system that doesn't just retrieve informationâ€”it thinks, verifies, and synthesizes like a team of expert analysts.**

---

## ğŸ¯ What Makes This Special?

This isn't another ChatGPT wrapper. This is a **cognitive architecture** where specialized AI agents collaborate to solve complex competitive intelligence challenges that single-model systems struggle with.

### The Problem We Solve

Traditional RAG systems fail when you ask:

> *"Source 1 says quantum supremacy was achieved in 2019. Source 2 claims it hasn't been practically achieved yet. Cross-reference with Nature, IBM, and Google to determine consensus."*

**Why?** Because they:
- âŒ Don't detect contradictions between sources
- âŒ Can't distinguish credible from unreliable information  
- âŒ Lack strategic decomposition of complex queries
- âŒ Provide answers without verification

**Our system handles this autonomously.**

---

## ğŸ—ï¸ Architecture: Five Specialized Agents
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Analyzer  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ Routing â”‚
    â””â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”˜
      â”‚    â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â” â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Researchâ”‚ â””â”€â”€â–ºâ”‚Doc Retrieval â”‚
â”‚ Agent  â”‚    â”‚   Agent      â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Fact Verification â”‚
    â”‚      Agent        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Synthesis    â”‚
      â”‚     Agent      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Final Answer + â”‚
      â”‚   Confidence   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§  **1. Query Analyzer Agent**
- **Role:** Strategic planning & task decomposition
- **Tech:** LLM-powered intent classification with Pydantic schema validation
- **Output:** Routes to research, retrieval, or parallel execution based on query complexity

**Example Analysis:**
```json
{
  "intent": "comparison",
  "routing_decision": "parallel",
  "tasks": [
    {"agent": "research_agent", "task": "Find latest NVIDIA H200 benchmarks"},
    {"agent": "doc_retrieval_agent", "task": "Retrieve AMD MI300X technical specs"}
  ]
}
```

---

### ğŸŒ **2. Research Agent**  
- **Role:** Real-time web intelligence gathering
- **Tech:** Tavily API + LLM-driven query expansion
- **Features:**
  - Credibility scoring (0.0-1.0) based on source authority
  - Recency weighting (2024 = 1.0, older = decay)
  - Structured finding extraction with source tracking

**Live Search Example:**
```python
findings = [
  {
    "content": "NVIDIA H200 delivers 1,979 TFLOPS FP8...",
    "source_type": "technical_spec",
    "credibility": 0.95,
    "recency": "2024"
  }
]
```

---

### ğŸ“š **3. Document Retrieval Agent**
- **Role:** Precision search across internal knowledge bases
- **Tech:** **Enhanced Hybrid Search** (our secret sauce ğŸ”¥)
  - **BM25 (25%):** Keyword matching with stemming & stopword removal
  - **Semantic Search (65%):** Vector similarity via Chroma + Nomic embeddings
  - **Cross-Encoder Reranking (10%):** Final relevance scoring
- **Advanced Features:**
  - LLM-powered query expansion (synonyms + technical terms)
  - Metadata boosting (recency, source type)
  - Diversity filtering (avoids redundant results)

**Performance:**
```
Traditional RAG â†’ Top-3 Recall: 0.39
Our Hybrid System â†’ Top-3 Recall: 0.76 (+97.7% improvement)
```

---

### âš–ï¸ **4. Fact Verification Agent**
- **Role:** Cross-source validation & contradiction detection
- **Intelligence:**
  - Groups similar facts across sources
  - Detects numeric discrepancies (e.g., "1,979 TFLOPS" vs "2,100 TFLOPS")
  - Identifies semantic contradictions ("increase" vs "decrease")
  - Calculates confidence scores with credibility weighting

**Verification Output:**
```python
{
  "verified_facts": 12,
  "contradictions_found": 2,
  "overall_confidence": 0.87,
  "contradiction_report": [
    {
      "content": "Performance benchmarks vary by 15% across sources",
      "sources_involved": ["nvidia.com", "independent_test"]
    }
  ]
}
```

---

### ğŸ§ª **5. Synthesis Agent**
- **Role:** Master analyst that combines everything
- **Intelligence:**
  - Intent-aware responses (comparison â†’ side-by-side, research â†’ narrative)
  - Acknowledges contradictions rather than hiding them
  - Provides sourced citations for every claim
  - Confidence scoring based on information quality

**Sample Output:**
```
Answer: Based on verified benchmarks, NVIDIA H200 achieves 1,979 TFLOPS 
(nvidia.com, credibility: 0.95), while AMD MI300X reaches 1,835 TFLOPS 
(amd.com, credibility: 0.85). Independent tests show 3-5% variance...

Confidence: 0.87
Citations: [3 sources]
Contradictions: Price discrepancies noted ($98.32/hr vs $85/hr for bulk)
```

---

## ğŸª Orchestration: LangGraph Workflow

We use **LangGraph** (not basic LangChain) for stateful, graph-based orchestration:

**Key Features:**
- âœ… **Conditional routing** based on query analysis
- âœ… **Parallel execution** (research + retrieval simultaneously)
- âœ… **State management** with checkpoints for debugging
- âœ… **Error isolation** (one agent fails â‰  system crash)

**Visual Workflow:**
```python
workflow = StateGraph(AgentState)
workflow.add_node("query_analyzer", analyzer.execute)
workflow.add_conditional_edges(
    "query_analyzer",
    route_logic,
    {
        "research_only": "research_agent",
        "retrieval_only": "doc_retrieval_agent", 
        "parallel": "parallel_branch"
    }
)
```

---

## ğŸ“Š Performance Metrics

![Performance Metrics](benchmarks/performance_comparison.png)

| Metric | Baseline RAG | Multi-Agent System | Improvement |
|--------|--------------|-------------------|-------------|
| **Context Recall** | 0.39 | 0.76 | **+97.7%** â¬†ï¸ |
| **Answer Accuracy (ROUGE)** | 0.17 | 0.30 | **+73.7%** â¬†ï¸ |
| **Faithfulness** | 0.40 | 0.66 | **+66.0%** â¬†ï¸ |
| **Relevancy** | 0.64 | 0.81 | **+26.9%** â¬†ï¸ |

*Evaluated on 50 complex competitive intelligence queries*

---

## ğŸ› ï¸ Tech Stack

### Core Framework
- **LangGraph**: State machine orchestration
- **LangChain**: Agent primitives & document loaders
- **Ollama**: Local LLM inference (Llama 3.1, Nemotron)

### Intelligence Layer
- **Tavily API**: Real-time web search
- **Chroma**: Vector database
- **Nomic Embeddings**: Semantic search
- **BM25**: Keyword retrieval

### Validation & Quality
- **Pydantic**: Schema validation for structured outputs
- **NLTK**: Text preprocessing (stemming, tokenization)
- **Custom**: Credibility scoring, contradiction detection

---

## ğŸš€ Quick Start

### Prerequisites
```bash
# Install Ollama
curl https://ollama.ai/install.sh | sh

# Pull models
ollama pull llama3.1:8b
ollama pull nomic-embed-text
```

### Installation
```bash
git clone https://github.com/yourusername/multi-agent-rag
cd multi-agent-rag
pip install -r requirements.txt

# Set up API keys
echo "TAVILY_API_KEY=your_key_here" > .env
```

### Run the System
```bash
# Index your knowledge base
python src/data_ingestion/ingest.py --source /path/to/docs

# Start the workflow
python src/orchestration/main.py
```

### Example Query
```python
from graph.workflow import MultiAgentGraph

workflow = MultiAgentGraph()
result = await workflow.process_query(
    "Compare NVIDIA H200 vs AMD MI300X for LLM training: performance, cost, availability"
)

print(f"Answer: {result['final_answer']}")
print(f"Confidence: {result['confidence_score']:.2%}")
print(f"Path: {' â†’ '.join(result['execution_path'])}")
```

**Output:**
```
Answer: Based on verified technical specifications and independent benchmarks...
Confidence: 87%
Path: query_analyzer â†’ parallel_branch â†’ fact_verification_agent â†’ synthesis_agent
```

---

## ğŸ“ Project Structure
```
multi-agent-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ query_analyzer.py          # Intent classification & routing
â”‚   â”‚   â”œâ”€â”€ research_agent.py          # Tavily web search integration
â”‚   â”‚   â”œâ”€â”€ doc_retrieval_agent.py     # Hybrid search retriever
â”‚   â”‚   â”œâ”€â”€ fact_verification_agent.py # Cross-source validation
â”‚   â”‚   â””â”€â”€ synthesis_agent.py         # Final answer generation
â”‚   â”œâ”€â”€ graph/
â”‚   â”‚   â”œâ”€â”€ workflow.py                # LangGraph orchestration
â”‚   â”‚   â””â”€â”€ state.py                   # Shared state schema
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ enhanced_hybrid_search.py  # Custom retrieval engine
â”‚   â””â”€â”€ orchestration/
â”‚       â””â”€â”€ main.py                    # Entry point
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vector_db/                     # Chroma persistence
â”œâ”€â”€ config/
â”‚   â””â”€â”€ agents.yaml                    # Configuration
â””â”€â”€ benchmarks/
    â””â”€â”€ golden_dataset.json            # Evaluation queries
```

---

## ğŸ”¬ Key Innovations

### 1. **Intent-Aware Routing**
Unlike static pipelines, our Query Analyzer dynamically decides:
- Should we search the web or internal docs?
- Can tasks run in parallel or must be sequential?
- What output format fits the question (table vs narrative)?

### 2. **Credibility-Weighted Verification**
Not all sources are equal. We score credibility based on:
- Domain authority (.edu = 0.9, .blog = 0.6)
- Content quality (citations, depth)
- Recency (exponential decay for time-sensitive topics)

### 3. **Contradiction Detection**
Two-pronged approach:
- **Numeric:** Variance analysis (15% difference in TFLOPS â†’ flag it)
- **Semantic:** Opposite term detection ("increase" vs "decrease")

### 4. **Enhanced Hybrid Retrieval**
Combines three retrieval paradigms with learned weights:
```python
final_score = (
    bm25_score Ã— 0.25 +      # Keyword precision
    semantic_score Ã— 0.65 +   # Meaning similarity
    reranker_score Ã— 0.10     # Contextual relevance
) Ã— metadata_boost Ã— length_penalty
```

---

## ğŸ§ª Evaluation Framework

We use **golden queries** to measure performance:

### Sample Queries
1. **Contradiction Resolution:**  
   *"Source 1 says X. Source 2 says Y. What's the consensus?"*
   
2. **Multi-Hop Reasoning:**  
   *"Find Tesla Dojo D1 specs, identify FP32 performance, verify against MLPerf benchmarks."*
   
3. **Trend Analysis:**  
   *"Chart RAG adoption mentions in Microsoft, Google, Salesforce earnings (2023-2024)."*

### Metrics
- **Faithfulness:** Does the answer match retrieved evidence?
- **Relevancy:** Are sources on-topic?
- **Recall:** Did we find all relevant documents?
- **ROUGE:** Lexical overlap with ground truth

**Run Evaluation:**
```bash
python benchmarks/evaluate.py --dataset golden_dataset.json
```

---

## ğŸ›£ï¸ Roadmap

### âœ… Phase 1: Foundation (Current)
- [x] Five-agent architecture
- [x] LangGraph orchestration  
- [x] Enhanced hybrid retrieval
- [x] Fact verification pipeline

### ğŸ”„ Phase 2: Production Hardening (Q2 2024)
- [ ] API deployment (FastAPI)
- [ ] Streaming responses
- [ ] User feedback loop
- [ ] Fine-tuned reranking models

### ğŸŒŸ Phase 3: Advanced Intelligence (Q3 2024)
- [ ] Memory system (conversation history)
- [ ] Multi-turn reasoning
- [ ] Chart/table generation
- [ ] Competitive moat analysis

---

## ğŸ¤ Contributing

We welcome contributions! Areas of focus:
- **Retrieval:** Better reranking models (cross-encoder fine-tuning)
- **Agents:** New specialist agents (e.g., visualization, forecasting)
- **Evaluation:** More golden queries + human relevance judgments

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## ğŸ“š Learn More

### Key Papers Implemented
- [RAG: Retrieval-Augmented Generation (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401)
- [Multi-Agent Systems in NLP (Andreas et al., 2023)](https://arxiv.org/abs/2305.14325)
- [Hybrid Search Best Practices (Robertson & Zaragoza, 2009)](https://www.microsoft.com/en-us/research/publication/the-probabilistic-relevance-framework-bm25-and-beyond/)

### Related Work
- **LangGraph Documentation:** [langgraph.docs](https://github.com/langchain-ai/langgraph)
- **Tavily Search API:** [tavily.com/docs](https://tavily.com)
- **Chroma Vector DB:** [trychroma.com](https://www.trychroma.com/)

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™‹ FAQ

**Q: Why not use GPT-4 API directly?**  
A: Local models (Ollama) give us:
- Zero inference costs
- Data privacy (no external API calls)
- Customization (fine-tune for our domain)

**Q: How does this compare to LlamaIndex?**  
A: LlamaIndex is a retrieval library. We built an **agentic reasoning system** where retrieval is one component. Think "system" vs "library."

**Q: Can I add my own agents?**  
A: Absolutely! Extend `BaseAgent` class, define your logic, register in `workflow.py`. Example:
```python
class PricingAnalysisAgent(BaseAgent):
    async def _execute_impl(self, state):
        # Your custom logic
        return state
```

**Q: What hardware do I need?**  
A: Minimum: 16GB RAM, no GPU required (CPU inference via Ollama).  
Recommended: 32GB RAM + NVIDIA GPU for faster processing.

--

## ğŸ”¥ Live Demo

Try the system with this query:
```bash
python src/orchestration/main.py --query \
  "Compare NVIDIA H200 and AMD MI300X: Which is better for training Llama 3 70B? \
   Consider TFLOPS, memory bandwidth, and AWS pricing."
```

**Output:**
```
ğŸ” QueryAnalyzer analyzing: Compare the RAG adoption velocity and primary stra...
SUCCESS
{'intent': 'comparison', 'tasks': [{'task': 'analyze_RAG_adoption_velocity', 'agent': 'doc_retrieval_agent', 'priority': 2, 'expected_output': 'A table comparing RAG adoption velocity for Microsoft and Salesforce between Q1 2023 and Q1 2024.'}, {'task': 'identify_peak_mention_volume', 'agent': 'research_agent', 'priority': 1, 'expected_output': 'A list of companies that reached peak mention volume first between Q1 2023 and Q1 2024.'}], 'metadata': {'complexity': 'moderate', 'requires_external_data': True, 'time_sensitivity': 'high', 'expected_output_format': 'table'}, 'routing_decision': 'parallel'}
--------------------------------------------------
âœ… Success: Intent=comparison, Tasks=2
âœ… [Graph] Query analysis complete
ğŸ”„ [Graph] Routing decision: parallel

âš¡ [Graph] Running Parallel Branch...
ğŸ” ResearchAgent conducting research with Tavily API...
No tasks found!!!
Going to default mode

   Found 1 research tasks
   Researching: Search for: Compare the RAG adoption velocity and primary st...
   Searching Tavily for: Search for: Compare the RAG adoption velocity and ...
   Found 10 search results
ğŸ” DocRetrievalAgent searching for relevant documents...
ğŸ” Enhanced hybrid search for: Compare the RAG adoption velocity and primary strategic drivers for Microsoft an...
   Found 1 findings for this task
âœ… Research completed: 1 findings, 1 sources
   Credibility: 0.50, Recency: 1.00
   Sample finding: Microsoft reached peak mention volume first....
ğŸ” Query expanded: 8 terms added
âœ… Enhanced retrieval: 5 documents
   Top score: 0.735
   Components - BM25: 1.000, Semantic: 0.635, Reranker: 0.087
âœ… Retrieved 5 documents
   Top result: 2. Microsoft: RAG Adoption Timeline and Signals 2.1 Quarterly RAG Mentions (Microsoft)  Microsoftâ€™s ...
   Metadata: {'source': 'financial_report'}
âœ… [Graph] Parallel branch complete

 âš–ï¸ [Graph] Running Fact Verification Agent...
ğŸ” FactVerificationAgent validating facts...
   Extracted 11 facts for verification
âœ… Verification complete:
   Verified facts: 2
   Contradictions found: 1
   Overall confidence: 0.60
   âš ï¸  Contradictions detected in 1 fact groups
     1. Microsoft: RAG Adoption Timeline and Signals
2.1 Quarterly RAG Mentions (Microso...
âœ… [Graph] Verification complete: 2 facts checked, 1 contradictions found

ğŸ§  [Graph] Running Synthesis Agent...
ğŸ§  SynthesisAgent synthesizing final answer...
â³ Invoking LLM (Intent: comparison)...
âœ… Synthesis complete. Confidence: 0.78
âœ… [Graph] Synthesis complete: Confidence=0.78

âœ… Graph execution complete
ğŸ“Š Final confidence: 0.78
ğŸš¶ Execution path: query_analyzer â†’ research_agent â†’ doc_retrieval_agent â†’ fact_verification_agent
total time to execute the query is 391.16793160000816
âœ… Answer generated: True
ğŸ“Š Confidence: 0.78
âš–ï¸  Verification: 2 facts, 1 contradictions
ğŸš¶ Path: query_analyzer â†’ research_agent â†’ doc_retrieval_agent â†’ fact_verification_agent â†’ synthesis_agent (inferred)
ğŸ“ answer: A comparison of RAG adoption velocity and primary strategic drivers between Microsoft and Salesforce reveals distinct patterns. According to the verified facts, Microsoft's earnings calls show the earliest and most aggressive adoption curve among the three companies. In Q1 2023, Microsoft had 2 RAG mentions, which correlated with the expansion of Azure OpenAI Service. The focus was on grounding LLMs with enterprise data. By Q3 2023, Microsoft reached a peak of 12 RAG mentions, attributed to the launch of Microsoft 365 Copilot. This major spike in RAG mentions directly correlates with flagship AI product launches, not research announcements alone.

In contrast, Salesforce's primary driver for RAG adoption was Einstein GPT GA, which led to a largest spike of 16 RAG mentions in Q3 2023. However, it is worth noting that Microsoft reached peak mention volume first, as indicated by the raw research findings. The contradiction between the two sources regarding the exact timing of Microsoft's peak mention volume is acknowledged.

A side-by-side analysis highlights strengths and weaknesses of each entity. Microsoft's aggressive adoption curve and focus on enterprise-wide copilots grounded in documents, emails, and SharePoint demonstrate its strategic commitment to RAG technology. Salesforce's emphasis on unified customer data as the RAG backbone showcases its efforts to integrate AI with existing infrastructure.

Specific metrics from the verified facts support these observations: Microsoft's Q3 2023 spike of 12 RAG mentions versus Salesforce's Q3 2023 largest spike of 16 RAG mentions. The credibility of these findings is supported by a source credibility score of 0.9..
```

---

<div align="center">

### â­ If this helps your research/project, please star the repo!

**Built with ğŸ§  by ABHIRUP SARKAR**


**Ready to build intelligent systems that think, verify, and synthesize?** 

Clone the repo and start experimenting! ğŸš€
```bash
git clone https://github.com/AbhiIITDhanbad/multi-agent-rag
cd multi-agent-rag
python src/orchestration/main.py
```

---

**Star â­ this repo if you found it useful!**
